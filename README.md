# ğŸš€ Despliegue de Modelos en la Nube - ConfiguraciÃ³n Ã“ptima

**Implementa modelos de IA en la nube con mÃ¡xima eficiencia**  
Este repositorio contiene la configuraciÃ³n esencial para desplegar servicios de embedding y modelos de lenguaje en entornos cloud, optimizado para GPUs de alto rendimiento (24GB+ VRAM). Incluye:

- ğŸ” GestiÃ³n segura de claves API y tokens
- ğŸ³ ConfiguraciÃ³n lista para Docker/RunPod
- ğŸ¤– Soporte para modelos de HuggingFace
- âš¡ Endpoints API con Uvicorn
- ğŸ“Š MonitorizaciÃ³n integrada

**Requisitos mÃ­nimos:** GPU con arquitectura CUDA â‰¥ 8.0 (recomendado RTX 3080/A4000+)

## ğŸ” ConfiguraciÃ³n del Entorno

### Archivo `.env`
```ini
API_KEY="your_API_key_here"  # Reemplazar con tu clave real

```
## ğŸ› ï¸ ConfiguraciÃ³n Inicial

# Navegar al directorio de trabajo
cd /workspace

# Generar clave SSH
ssh-keygen -t ed25519 -C "igmemarcial@gmail.com"
cat ~/.ssh/id_ed25519.pub

# Clonar repositorio (2 mÃ©todos)
git clone https://igmeMarcial:ghp_ABC123xyz456@github.com/IgmeAI/igme-models.git
# O
git clone git@github.com:IgmeAI/igme-models.git
cd igme-models


## ğŸ’¾ ConfiguraciÃ³n de CachÃ©

# Crear directorio para cachÃ© de HuggingFace
mkdir -p /workspace/huggingface_cache

# Configurar variables de entorno (ejecutar en cada nueva terminal)
export HF_HOME=/workspace/huggingface_cache
export HF_TOKEN="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"


## âš™ï¸ InstalaciÃ³n

# Opcional: Crear entorno virtual
# python -m venv venv
# source venv/bin/activate

# Instalar dependencias
pip install -r requirements.txt

# AutenticaciÃ³n con HuggingFace
huggingface-cli login


## ğŸš€ EjecuciÃ³n

uvicorn main:app --host 0.0.0.0 --port 8000



## ğŸ” VerificaciÃ³n

- Comprobar GPU:
https://eio4zvjfnv5vdl-8000.proxy.runpod.net/gpu-check

- DocumentaciÃ³n API:
https://eio4zvjfnv5vdl-8000.proxy.runpod.net/docs
